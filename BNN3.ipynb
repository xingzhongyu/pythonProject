{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import GPy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def log_gaussian_loss(output, target, sigma, no_dim):\n",
    "    exponent = -0.5*(target - output)**2/sigma**2\n",
    "    log_coeff = -no_dim*torch.log(sigma) - 0.5*no_dim*np.log(2*np.pi)\n",
    "\n",
    "    return - (log_coeff + exponent).sum()\n",
    "\n",
    "\n",
    "def get_kl_divergence(weights, prior, varpost):\n",
    "    prior_loglik = prior.loglik(weights)\n",
    "\n",
    "    varpost_loglik = varpost.loglik(weights)\n",
    "    varpost_lik = varpost_loglik.exp()\n",
    "\n",
    "    return (varpost_lik*(varpost_loglik - prior_loglik)).sum()\n",
    "\n",
    "\n",
    "class gaussian:\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def loglik(self, weights):\n",
    "        exponent = -0.5*(weights - self.mu)**2/self.sigma**2\n",
    "        log_coeff = -0.5*(np.log(2*np.pi) + 2*np.log(self.sigma))\n",
    "\n",
    "        return (exponent + log_coeff).sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class MC_Dropout_Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_units, drop_prob):\n",
    "        super(MC_Dropout_Model, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        # network with two hidden and one output layer\n",
    "        self.layer1 = nn.Linear(input_dim, num_units)\n",
    "        self.layer2 = nn.Linear(num_units, 2*output_dim)\n",
    "\n",
    "        self.activation = nn.ReLU(inplace = True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.view(-1, self.input_dim)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = F.dropout(x, p=self.drop_prob, training=True)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class MC_Dropout_Wrapper:\n",
    "    def __init__(self, network, learn_rate, batch_size, weight_decay):\n",
    "\n",
    "        self.learn_rate = learn_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.network = network\n",
    "        # self.network.cuda()\n",
    "\n",
    "        self.optimizer = torch.optim.SGD(self.network.parameters(), lr=learn_rate, weight_decay=weight_decay)\n",
    "        self.loss_func = log_gaussian_loss\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        x, y = x, y\n",
    "\n",
    "        # reset gradient and total loss\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        output = self.network(x)\n",
    "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def get_loss_and_rmse(self, x, y, num_samples):\n",
    "        x, y = x, y\n",
    "\n",
    "        means, stds = [], []\n",
    "        for i in range(num_samples):\n",
    "            output = self.network(x)\n",
    "            means.append(output[:, :1])\n",
    "            stds.append(output[:, 1:].exp())\n",
    "\n",
    "        means, stds = torch.cat(means, dim=1), torch.cat(stds, dim=1)\n",
    "        mean = means.mean(dim=-1)[:, None]\n",
    "        std = ((means.var(dim=-1) + stds.mean(dim=-1)**2)**0.5)[:, None]\n",
    "        loss = self.loss_func(mean, y, std, 1)\n",
    "\n",
    "        rmse = ((mean - y)**2).mean()**0.5\n",
    "\n",
    "        return loss.detach().cpu(), rmse.detach().cpu()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 1) (250, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Type must be a sub-type of ndarray type",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 30\u001B[0m\n\u001B[0;32m     26\u001B[0m nets, losses \u001B[38;5;241m=\u001B[39m [], []\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[1;32m---> 30\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m200\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     33\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;132;01m%4d\u001B[39;00m\u001B[38;5;124m, Train loss = \u001B[39m\u001B[38;5;132;01m%7.3f\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m (i, loss\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m/\u001B[39mbatch_size))\n",
      "Cell \u001B[1;32mIn[8], line 19\u001B[0m, in \u001B[0;36mMC_Dropout_Wrapper.fit\u001B[1;34m(self, x, y)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# reset gradient and total loss\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 19\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnetwork\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_func(output[:, :\u001B[38;5;241m1\u001B[39m], y, output[:, \u001B[38;5;241m1\u001B[39m:]\u001B[38;5;241m.\u001B[39mexp(), \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     22\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[5], line 18\u001B[0m, in \u001B[0;36mMC_Dropout_Model.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 18\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput_dim\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer1(x)\n\u001B[0;32m     21\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivation(x)\n",
      "\u001B[1;31mValueError\u001B[0m: Type must be a sub-type of ndarray type"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "no_points = 400\n",
    "lengthscale = 1\n",
    "variance = 1.0\n",
    "sig_noise = 0.3\n",
    "x = np.random.uniform(-3, 3, no_points)[:, None]\n",
    "x.sort(axis = 0)\n",
    "\n",
    "\n",
    "k = GPy.kern.RBF(input_dim=1, variance=variance, lengthscale=lengthscale)\n",
    "C = k.K(x, x) + np.eye(no_points)*(x + 2)**2*sig_noise**2\n",
    "\n",
    "y = np.random.multivariate_normal(np.zeros((no_points)), C)[:, None]\n",
    "y = (y - y.mean())\n",
    "x_train = x[75:325]\n",
    "y_train = y[75:325]\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "num_epochs, batch_size = 2000, len(x_train)\n",
    "\n",
    "net = MC_Dropout_Wrapper(network=MC_Dropout_Model(input_dim=1, output_dim=1, num_units=200, drop_prob=0.5),\n",
    "                         learn_rate=1e-4, batch_size=batch_size, weight_decay=1e-2)\n",
    "\n",
    "fit_loss_train = np.zeros(num_epochs)\n",
    "best_net, best_loss = None, float('inf')\n",
    "nets, losses = [], []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    loss = net.fit(x_train, y_train)\n",
    "\n",
    "    if i % 200 == 0:\n",
    "        print('Epoch: %4d, Train loss = %7.3f' % (i, loss.cpu().data.numpy()/batch_size))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "samples = []\n",
    "noises = []\n",
    "for i in range(1000):\n",
    "    preds = net.network.forward(torch.linspace(-5, 5, 200).cuda()).cpu().data.numpy()\n",
    "    samples.append(preds[:, 0])\n",
    "    noises.append(np.exp(preds[:, 1]))\n",
    "\n",
    "samples = np.array(samples)\n",
    "noises = np.array(noises)\n",
    "means = (samples.mean(axis = 0)).reshape(-1)\n",
    "\n",
    "aleatoric = (noises**2).mean(axis = 0)**0.5\n",
    "epistemic = (samples.var(axis = 0)**0.5).reshape(-1)\n",
    "total_unc = (aleatoric**2 + epistemic**2)**0.5\n",
    "\n",
    "\n",
    "c = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
    "     '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "\n",
    "plt.figure(figsize = (6, 5))\n",
    "plt.style.use('default')\n",
    "plt.scatter(x_train, y_train, s = 10, marker = 'x', color = 'black', alpha = 0.5)\n",
    "plt.fill_between(np.linspace(-5, 5, 200), means + aleatoric, means + total_unc, color = c[0], alpha = 0.3, label = 'Epistemic + Aleatoric')\n",
    "plt.fill_between(np.linspace(-5, 5, 200), means - total_unc, means - aleatoric, color = c[0], alpha = 0.3)\n",
    "plt.fill_between(np.linspace(-5, 5, 200), means - aleatoric, means + aleatoric, color = c[1], alpha = 0.4, label = 'Aleatoric')\n",
    "plt.plot(np.linspace(-5, 5, 200), means, color = 'black', linewidth = 1)\n",
    "plt.xlim([-5, 5])\n",
    "plt.ylim([-5, 7])\n",
    "plt.xlabel('$x$', fontsize=30)\n",
    "plt.title('MC dropout', fontsize=40)\n",
    "plt.tick_params(labelsize=30)\n",
    "plt.xticks(np.arange(-4, 5, 2))\n",
    "plt.yticks(np.arange(-4, 7, 2))\n",
    "plt.gca().set_yticklabels([])\n",
    "plt.gca().yaxis.grid(alpha=0.3)\n",
    "plt.gca().xaxis.grid(alpha=0.3)\n",
    "plt.savefig('mc_dropout_hetero.pdf', bbox_inches = 'tight')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
